{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-15T22:51:34.278369Z",
     "start_time": "2025-03-15T22:51:34.272913Z"
    }
   },
   "source": [
    "# Read the current README.md file\n",
    "with open('README.md', 'r') as f:\n",
    "    current_readme = f.read()\n",
    "\n",
    "print(\"Current README.md content:\")\n",
    "print(current_readme)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current README.md content:\n",
      "# Scraper\n",
      "\n",
      "## Objectives\n",
      "* Given a URL, recursively crawl its links\n",
      "  * Store the response\n",
      "  * Parse the response extracting new links\n",
      "  * Visit each link and repeat the operations above\n",
      "* Cache the results to avoid duplicative requests\n",
      "* Optionally, specify the maximum recursion depth\n",
      "* Optionally, specify whether to allow requests to other subdomains or domains\n",
      "* Optimize the process to leverage all available processors\n",
      "\n",
      "## Design\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T22:51:45.014066Z",
     "start_time": "2025-03-15T22:51:45.010736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Updated README content with design section\n",
    "updated_readme = \"\"\"# Scraper\n",
    "\n",
    "## Objectives\n",
    "* Given a URL, recursively crawl its links\n",
    "  * Store the response\n",
    "  * Parse the response extracting new links\n",
    "  * Visit each link and repeat the operations above\n",
    "* Cache the results to avoid duplicative requests\n",
    "* Optionally, specify the maximum recursion depth\n",
    "* Optionally, specify whether to allow requests to other subdomains or domains\n",
    "* Optimize the process to leverage all available processors\n",
    "\n",
    "## Design\n",
    "\n",
    "### 1. Architecture Components\n",
    "\n",
    "The project will be structured with these core components:\n",
    "\n",
    "1. **Crawler** - Main component that orchestrates the crawling process\n",
    "2. **RequestHandler** - Handles HTTP requests with proper headers, retries, and timeouts\n",
    "3. **ResponseParser** - Parses HTML responses to extract links\n",
    "4. **Cache** - Stores visited URLs and their responses\n",
    "5. **LinkFilter** - Filters links based on domain/subdomain rules\n",
    "6. **TaskManager** - Manages parallel execution of crawling tasks\n",
    "\n",
    "### 2. Caching Strategy\n",
    "\n",
    "For the caching requirement:\n",
    "\n",
    "- **In-memory cache**: Fast but limited by available RAM\n",
    "- **File-based cache**: Persistent but slower\n",
    "- **Database cache**: Structured and persistent, but requires setup\n",
    "\n",
    "We'll start with a simple in-memory cache using Python's built-in `dict` for development, then expand to a persistent solution like SQLite for production use.\n",
    "\n",
    "### 3. Concurrency Model\n",
    "\n",
    "For optimizing to leverage all available processors:\n",
    "\n",
    "- **Threading**: Good for I/O bound operations like web requests\n",
    "- **Multiprocessing**: Better for CPU-bound tasks\n",
    "- **Async I/O**: Excellent for many concurrent I/O operations\n",
    "\n",
    "We'll use `asyncio` with `aiohttp` for making concurrent requests, as web scraping is primarily I/O bound.\n",
    "\n",
    "### 4. URL Handling and Filtering\n",
    "\n",
    "For domain/subdomain filtering:\n",
    "- Use `urllib.parse` to extract and compare domains\n",
    "- Implement a configurable rule system (allow/deny lists)\n",
    "- Handle relative URLs properly by converting them to absolute\n",
    "\n",
    "### 5. Depth Management\n",
    "\n",
    "For recursion depth:\n",
    "- Track depth as a parameter passed to each recursive call\n",
    "- Implement a max depth check before proceeding with crawling\n",
    "- Consider breadth-first vs. depth-first strategies\n",
    "\n",
    "### 6. Error Handling & Politeness\n",
    "\n",
    "Additional considerations:\n",
    "- Robust error handling for network issues and malformed HTML\n",
    "- Rate limiting to avoid overwhelming servers\n",
    "- Respect for `robots.txt` rules\n",
    "- User-agent identification\n",
    "\n",
    "### 7. Data Storage\n",
    "\n",
    "For storing the crawled data:\n",
    "- Define a clear structure for storing URLs and their associated content\n",
    "- Consider what metadata to keep (status code, headers, timestamps)\n",
    "\"\"\"\n",
    "\n",
    "# Write the updated content to README.md\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(updated_readme)\n",
    "\n",
    "print(\"README.md has been updated successfully!\")"
   ],
   "id": "e4f156d4e1a94c3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md has been updated successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
